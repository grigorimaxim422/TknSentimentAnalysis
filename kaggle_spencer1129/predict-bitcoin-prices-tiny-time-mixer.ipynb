{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"ffb2575f08555de19f22a62be6b2b65532b71343defd38fca29c358510464fc3","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10133645,"sourceType":"datasetVersion","datasetId":6254189}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predict Bitcoin Close Prices using Foundational Models**\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n","metadata":{}},{"cell_type":"markdown","source":"This project is focused on **Time Series Forecasting** using the **Tiny Time Mixer (TTM)** foundation model provided by IBM. We will download and prepare the dataset, set up the environment for the `tsfm` model, and perform forecasting tasks.\n\nRefer to the following links for detailed documentation and resources:\n\n- [IBM Granite TSFM Repository](https://github.com/ibm-granite/granite-tsfm/tree/main)\n- [IBM Granite Time Series Documentation](https://www.ibm.com/granite/docs/models/time-series/?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-TSFM_FINAL-v1_1729695227)\n- [IBM Foundation Model Time Series Forecasting Tutorial](https://developer.ibm.com/tutorials/awb-foundation-model-time-series-forecasting/#step-1-set-up-your-environment3) from Joshua Noble.\n","metadata":{}},{"cell_type":"markdown","source":"![Forecast Process](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/vpEkXD--XRkEbnkiZZOfYw/image-1%20-1-.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='objectives'></a>[Objectives](#toc)\n\nBy the end of this project, you will be able to:\n\n1. **Install and set up the environment** for time series forecasting using IBM’s Tiny Time Mixer (TTM) model.\n   \n2. **Download and prepare the dataset**: Download the Bitcoin historical data and perform the necessary data preprocessing.\n\n3. **Understand the architecture of the TTM model**: Learn how the TTM model is designed for efficient time series forecasting, leveraging its specialized architecture for temporal data.\n\n4. **Train and evaluate the model**: Train the TTM model using the processed dataset, then use it to forecast Bitcoin prices on test data, and visualize the results.\n\n5. **Perform model evaluation**: Analyze the performance of the TTM model by comparing actual and predicted prices using various metrics and visualizations.\n\n6. **Customize and fine-tune**: Explore options for fine-tuning the model and experimenting with different time series datasets for extended forecasting capabilities.\n\n7. **Leverage the IBM Granite repository**: Gain practical experience with IBM’s Granite repository for handling real-world forecasting tasks, enabling you to apply the model to other time series forecasting projects.\n","metadata":{}},{"cell_type":"markdown","source":"----\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='setup'></a>[Setup](#toc)\n\nFor this project, we will be using the following libraries:\n\n*  `pandas` for data manipulation and analysis, specifically for handling time series data.\n*  `numpy` for numerical computations and array operations.\n*  `matplotlib` for visualizing time series trends and the forecasting results.\n*  `granite` provides the **Tiny Time Mixer (TTM)** model for time series forecasting.\n*  `os` for interacting with the file system and setting up the project environment.\n\nEnsure that these libraries are installed and properly configured before proceeding with the forecasting tasks.\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='installing-required-libraries'></a>[Installing required libraries](#toc)\n\n\n\nThis step could take **several minutes**, please be patient.\n\n","metadata":{}},{"cell_type":"code","source":"import requests\n\n# URL of the models ZIP file\nurl = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8Ptx0Quj9P2MoYqLrv5X7g/models.zip'\n\n# Download the models file\nresponse = requests.get(url)\n\n\n# Save the file locally\nzip_filename = 'models.zip'\nwith open(zip_filename, 'wb') as f:\n    f.write(response.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:23:00.081028Z","iopub.execute_input":"2024-12-08T01:23:00.082328Z","iopub.status.idle":"2024-12-08T01:23:01.028376Z","shell.execute_reply.started":"2024-12-08T01:23:00.082266Z","shell.execute_reply":"2024-12-08T01:23:01.027230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip -o models.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:25:17.477407Z","iopub.execute_input":"2024-12-08T01:25:17.477941Z","iopub.status.idle":"2024-12-08T01:25:18.875105Z","shell.execute_reply.started":"2024-12-08T01:25:17.477893Z","shell.execute_reply":"2024-12-08T01:25:18.873670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='cloning-the-ibm-tiny-time-mixer-ttm-repository'></a>[Cloning the IBM Tiny Time Mixer (TTM) repository](#toc)\n\nWe need to clone the **IBM Tiny Time Mixer (TTM)** repository, which contains the pre-trained model and utilities for time series forecasting.\n","metadata":{}},{"cell_type":"code","source":"# Clone the ibm/tsfm\n! [ -d \"tsfm\" ] && rm -rf tsfm\n! git clone --depth 1 --branch v0.2.9 https://github.com/IBM/tsfm.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:23:41.993652Z","iopub.execute_input":"2024-12-08T01:23:41.994045Z","iopub.status.idle":"2024-12-08T01:23:45.634054Z","shell.execute_reply.started":"2024-12-08T01:23:41.994014Z","shell.execute_reply":"2024-12-08T01:23:45.632680Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Installing other required libraries. Will take **~6 minutes** to install the libraries. Please be patient.\n","metadata":{}},{"cell_type":"code","source":"# Change directory. Move inside the tsfm repo.\n%cd tsfm\n\n# Install the tsfm library\n! pip install \".[notebooks]\" seaborn==0.13.2\n\n%cd ../\n\nprint(\"All the required libraries are installed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" !pip install urllib3  --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='importing-required-libraries'></a>[Importing required libraries](#toc)\n\nWe will now import all the necessary libraries for this project, which include libraries for numerical computations, deep learning, and time series forecasting.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport tempfile\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\n# TSFM libraries\nfrom tsfm_public.toolkit.time_series_forecasting_pipeline import TimeSeriesForecastingPipeline\nfrom tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\nfrom tsfm_public.toolkit.callbacks import TrackingCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:30:12.291844Z","iopub.execute_input":"2024-12-08T01:30:12.293456Z","iopub.status.idle":"2024-12-08T01:30:13.806609Z","shell.execute_reply.started":"2024-12-08T01:30:12.293403Z","shell.execute_reply":"2024-12-08T01:30:13.805449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <a id='data-loading-and-preprocessing'></a>[Data loading and preprocessing](#toc)\n\nWe use the pandas library to load the Bitcoin historical dataset, which contains minute-level trading data for Bitcoin. The data is stored in a CSV file named `btcusd_1-min_data.csv`.\n\nWe will now load the Bitcoin dataset and inspect the first few rows of the data using pandas `.head()` function.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data = pd.read_csv('/kaggle/input/btc-usd/btcusd-data.csv')\nbitcoin_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:07:44.195970Z","iopub.execute_input":"2024-12-08T02:07:44.196425Z","iopub.status.idle":"2024-12-08T02:07:51.535382Z","shell.execute_reply.started":"2024-12-08T02:07:44.196390Z","shell.execute_reply":"2024-12-08T02:07:51.534336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='dataset-description'></a>[Dataset description](#toc)\n\nThe `info()` function provides a concise summary of the dataset, including the number of non-null entries, data types, and memory usage. Below is a breakdown of the dataset:\n\nThe dataset contains historical data about Bitcoin prices. Each row represents Bitcoin's trading information for a specific minute, including the timestamp and various price metrics. Below is a description of the columns:\n\n| Column    | Type   | Description                                                                                       |\n|-----------|--------|---------------------------------------------------------------------------------------------------|\n| Timestamp | Integer| Unix timestamp representing the specific time of recording. Later converted to human-readable format. |\n| Open      | Float  | The price of Bitcoin at the start of the minute.                                                  |\n| High      | Float  | The highest price that Bitcoin reached during the minute.                                         |\n| Low       | Float  | The lowest price that Bitcoin fell to during the minute.                                          |\n| Close     | Float  | The price of Bitcoin at the end of the minute, typically the target variable for forecasting.     |\n| Volume    | Float  | The total volume of Bitcoin traded during that minute.                                            |\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:05.360700Z","iopub.execute_input":"2024-12-08T02:08:05.361586Z","iopub.status.idle":"2024-12-08T02:08:05.390169Z","shell.execute_reply.started":"2024-12-08T02:08:05.361544Z","shell.execute_reply":"2024-12-08T02:08:05.388792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the number of entries in the dataset\nlen(bitcoin_data)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='checking-for-missing-values'></a>[Checking for missing values](#toc)\n\nTo ensure the quality of our data, we need to check for any missing values in the dataset. This step is crucial because missing data can negatively impact the performance of our time series forecasting model.\n\n**Handling Missing Data:** Missing values are common in time series datasets. We can use the `isna()` function to detect missing entries, and `sum()` to count how many missing values exist in each column.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:15.295536Z","iopub.execute_input":"2024-12-08T02:08:15.296223Z","iopub.status.idle":"2024-12-08T02:08:15.370687Z","shell.execute_reply.started":"2024-12-08T02:08:15.296191Z","shell.execute_reply":"2024-12-08T02:08:15.369489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We observe that there is 1 missing value in the `Timestamp` column, while all other columns are complete. We will handle this missing timestamp appropriately. But first, let's have a look at the `Timestamp` column.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data['Timestamp']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:19.521968Z","iopub.execute_input":"2024-12-08T02:08:19.522424Z","iopub.status.idle":"2024-12-08T02:08:19.531776Z","shell.execute_reply.started":"2024-12-08T02:08:19.522390Z","shell.execute_reply":"2024-12-08T02:08:19.530730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output of accessing the `Timestamp` column shows that the last row (`6694280`) contains a missing value (`NaN`).\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='handling-missing-timestamps'></a>[Handling missing timestamps](#toc)\n\nTo handle the missing value in the `Timestamp` column, we use the `ffill()` method. This method forward-fills the missing value by propagating the last valid observation forward, ensuring that there are no gaps in the time series data.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data['Timestamp'] = bitcoin_data['Timestamp'].ffill()\nbitcoin_data['Timestamp']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:26.217268Z","iopub.execute_input":"2024-12-08T02:08:26.217699Z","iopub.status.idle":"2024-12-08T02:08:26.294702Z","shell.execute_reply.started":"2024-12-08T02:08:26.217663Z","shell.execute_reply":"2024-12-08T02:08:26.293553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Just to be sure, we can check if there are any remaining `NaN` values in the dataset.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:36.736860Z","iopub.execute_input":"2024-12-08T02:08:36.737597Z","iopub.status.idle":"2024-12-08T02:08:36.812182Z","shell.execute_reply.started":"2024-12-08T02:08:36.737557Z","shell.execute_reply":"2024-12-08T02:08:36.810783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='converting-timestamp-to-datetime-format'></a>[Converting 'Timestamp' to DateTime format](#toc)\n\nThe `Timestamp` column is currently in UNIX time format. To make it easier to work with and understand, we will convert the `Timestamp` column into a human-readable datetime format using the `pd.to_datetime` function.\n","metadata":{}},{"cell_type":"code","source":"bitcoin_data['Timestamp'] = pd.to_datetime(bitcoin_data['Timestamp'], unit='s')\nbitcoin_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:42.930287Z","iopub.execute_input":"2024-12-08T02:08:42.930695Z","iopub.status.idle":"2024-12-08T02:08:46.167210Z","shell.execute_reply.started":"2024-12-08T02:08:42.930663Z","shell.execute_reply":"2024-12-08T02:08:46.166140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='resampling-the-dataset-to-reduce-size'></a>[Resampling the dataset to reduce size](#toc)\n\nSince the original dataset contains minute-level data, it can be too large and time-consuming to train on. To address this issue, we will **resample** the dataset to **hourly data** by averaging the values over each hour. This will reduce the size of the dataset and speed up the training process, while still retaining the essential time-series patterns.\n\nThe `resample()` function in pandas allows us to aggregate data over specific time intervals. In this case, we are resampling the data to hourly intervals (`'1h'`) using the `Timestamp` column as the time reference. The `mean()` function is applied to compute the average value for each hour.\n\n- **Input**: The `Timestamp` column to use for resampling, along with the `'1h'` interval.\n- **Output**: A pandas DataFrame containing hourly-aggregated Bitcoin data.\n\nAdditionally, we use the following methods:\n- **dropna()**: To remove any rows containing missing values after resampling.\n- **reset_index()**: To reset the index after resampling and ensure that the DataFrame structure is intact.\n\nThis resampling step significantly reduces the dataset size, making it more efficient for training the time series forecasting model without losing key temporal patterns.\n","metadata":{}},{"cell_type":"code","source":"bt_data_resampled = bitcoin_data.resample('1h', on='Timestamp').mean().dropna().reset_index()\nbt_data_resampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:08:59.881740Z","iopub.execute_input":"2024-12-08T02:08:59.882273Z","iopub.status.idle":"2024-12-08T02:09:01.706269Z","shell.execute_reply.started":"2024-12-08T02:08:59.882225Z","shell.execute_reply":"2024-12-08T02:09:01.704968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='checking-for-missing-values-and-dataset-size-after-resampling'></a>[Checking for missing values and dataset size after resampling](#toc)\n\nAfter resampling the dataset to hourly intervals, we perform a check to ensure there are no missing values in the resampled data.\n","metadata":{}},{"cell_type":"code","source":"print(f\"Checking for NA values after resampling:\\n{bt_data_resampled.isna().sum()}\\n\")\nprint(f\"Number of entries after resampling: {len(bt_data_resampled)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:06.807188Z","iopub.execute_input":"2024-12-08T02:09:06.808067Z","iopub.status.idle":"2024-12-08T02:09:06.816696Z","shell.execute_reply.started":"2024-12-08T02:09:06.808026Z","shell.execute_reply":"2024-12-08T02:09:06.815541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As seen from the output, there are no missing values in the resampled dataset.\n\nDataset size comparison:\n\n| Dataset Level       | Number of Entries | Description                                 |\n|---------------------|-------------------|---------------------------------------------|\n| Before Resampling   | 6,694,281         | Original dataset at minute-level granularity. |\n| After Resampling    | 111,586           | Resampled dataset at hourly-level granularity. |\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='data-preparation'></a>[Data preparation](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='preparing-data-for-zero-shot-prediction-and-fine-tuning'></a>[Preparing data for zero-shot prediction and fine-tuning](#toc)\n\nNext, we prepare our data to perform **zero-shot prediction** and later **fine-tuning**. In this context, we are defining the key columns from the dataset that will be used for the model's prediction task.\n\n- **Timestamp column**: We use the `\"Timestamp\"` column as the time reference for our time series data.\n\n- **Target column**: The `[\"Close\"]` column will be our **target variable**. This represents the Bitcoin close price, which is the value we want to predict using our model.\n\n- **Observable columns**: The columns `[\"Open\", \"High\", \"Low\"]` are used as **observable variables**. The `Volume` column was left out from the observable columns because volume data does not directly affect price movement in the same way as the price-related variables.\n\n#### What is zero-shot learning?\n\n**Zero-shot learning** refers to the ability of a model to make predictions on tasks it has not been specifically trained for. In this context, the **Tiny Time Mixer** model can predict the **Bitcoin close price** without having seen any previous examples from this specific dataset. \n\nBy defining the timestamp, target, and observable columns, we are setting up the data for the next phase, where we will apply zero-shot prediction and fine-tuning with the **Tiny Time Mixer** model.\n","metadata":{}},{"cell_type":"code","source":"timestamp_column = \"Timestamp\"\ntarget_columns = [\"Close\"]\nobservable_columns = [\"Open\",\"High\",\"Low\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:23.169390Z","iopub.execute_input":"2024-12-08T02:09:23.169830Z","iopub.status.idle":"2024-12-08T02:09:23.175580Z","shell.execute_reply.started":"2024-12-08T02:09:23.169768Z","shell.execute_reply":"2024-12-08T02:09:23.174182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set seed for reproducibility\nSEED = 42\nset_seed(SEED)\n\n# Forecasting parameters\ncontext_length = 512 # TTM can use 512 time points into the past\nforecast_length = 96 # TTM can predict 96 time points into the future","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:28.883534Z","iopub.execute_input":"2024-12-08T02:09:28.883986Z","iopub.status.idle":"2024-12-08T02:09:28.903106Z","shell.execute_reply.started":"2024-12-08T02:09:28.883950Z","shell.execute_reply":"2024-12-08T02:09:28.901921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='splitting-the-dataset-for-training-validation-and-testing'></a>[Splitting the dataset for training, validation, and testing](#toc)\n\nWe split our resampled dataset into **training**, **validation**, and **testing** sets. This is essential for evaluating the model's performance at each stage of training.\n\nExplanation of the split:\n\n- **Training Set**: The first 80% of the data is used to train the model.\n- **Validation Set**: The validation period starts just after the training data. We shift the evaluation start back by the `context_length` to ensure the model has enough historical data for making predictions.\n- **Test Set**: The final 10% of the data is reserved for testing. Similar to the validation set, the test start is also shifted back by the `context_length`.\n\nBy defining this split configuration, we ensure that the model is trained and evaluated using separate time periods, which allows for reliable performance assessment.\n","metadata":{}},{"cell_type":"code","source":"from tsfm_public import (\n    TimeSeriesPreprocessor,\n    TinyTimeMixerForPrediction,\n    get_datasets,\n)\n\n# Get the length of the resampled data\ndata_length = len(bt_data_resampled)\n\n# Define the indices for the train, validation, and test splits\ntrain_start_index = 0\ntrain_end_index = round(data_length * 0.8)  # First 80% for training\n\n# Shift the start of evaluation back by the context length for proper prediction\neval_start_index = round(data_length * 0.8) - context_length  # Next 10% for validation\neval_end_index = round(data_length * 0.9)\n\n# Same adjustment for the test set\ntest_start_index = round(data_length * 0.9) - context_length  # Final 10% for testing\ntest_end_index = data_length\n\n# Store the split configuration for easy access\nsplit_config = {\n    \"train\": [train_start_index, train_end_index],\n    \"valid\": [eval_start_index, eval_end_index],\n    \"test\": [test_start_index, test_end_index],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:32.421491Z","iopub.execute_input":"2024-12-08T02:09:32.422559Z","iopub.status.idle":"2024-12-08T02:09:32.429559Z","shell.execute_reply.started":"2024-12-08T02:09:32.422520Z","shell.execute_reply":"2024-12-08T02:09:32.428307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"train_start_index: {train_start_index}, train_end_index: {train_end_index}\")\nprint(f\"eval_start_index: {eval_start_index}, eval_end_index: {eval_end_index}\")\nprint(f\"test_start_index: {test_start_index}, test_start_index: {test_end_index}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:40.861309Z","iopub.execute_input":"2024-12-08T02:09:40.861742Z","iopub.status.idle":"2024-12-08T02:09:40.868097Z","shell.execute_reply.started":"2024-12-08T02:09:40.861704Z","shell.execute_reply":"2024-12-08T02:09:40.866881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='specifying-columns-and-configuring-the-timeseriespreprocessor'></a>[Specifying columns and configuring the TimeSeriesPreprocessor](#toc)\n\nWe now define the key columns for the time series forecasting task and set up the `TimeSeriesPreprocessor` to standardize the data, which is essential for accurate predictions.\n\n**Column specifiers**:\n\n- **timestamp_column**: The `Timestamp` column, which marks the time for each data entry.\n- **target_columns**: The `Close` column, representing the value we aim to predict.\n- **observable_columns**: The `Open`, `High`, and `Low` columns, providing additional context for the model.\n\n#### What is TimeSeriesPreprocessor?\n\nThe `TimeSeriesPreprocessor` is a utility that prepares the time series data for the model by scaling it and setting context and prediction lengths. Specifically, it:\n\n- **Scales the data** using a standard scaler to normalize values.\n- **Sets context length** to 512 (past time points) and **prediction length** to 96 (future time points).\n- **Handles scaling** while leaving out categorical encoding since our data is numerical.\n\nAfter configuring the `TimeSeriesPreprocessor`, we use it to generate training, validation, and test datasets, ready for model training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"# Define the column specifiers for the time series data\ncolumn_specifiers = {\n    \"timestamp_column\": timestamp_column,  # Time reference column\n    \"target_columns\": target_columns,  # Target variable (Close price)\n    \"observable_columns\": observable_columns  # Observable variables (Open, High, Low prices)\n}\n\n# Configure the TimeSeriesPreprocessor\ntsp = TimeSeriesPreprocessor(\n    **column_specifiers,\n    context_length=context_length,  # Model will use 512 past time points\n    prediction_length=forecast_length,  # Model will predict 96 future time points\n    scaling=True,  # Apply scaling to normalize the data\n    encode_categorical=False,  # No categorical encoding needed\n    scaler_type=\"standard\"  # Use standard scaling\n)\n\n# Generate training, validation, and test datasets\n# This method returns torch vectors for training and validation\ntrain_dataset, valid_dataset, test_dataset = get_datasets(\n    tsp, bt_data_resampled, split_config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:09:49.489385Z","iopub.execute_input":"2024-12-08T02:09:49.489814Z","iopub.status.idle":"2024-12-08T02:09:50.125298Z","shell.execute_reply.started":"2024-12-08T02:09:49.489765Z","shell.execute_reply":"2024-12-08T02:09:50.124339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <a id='loading-and-evaluation-of-zero-shot-model'></a>[Loading and evaluation of zero-shot model](#toc)\n\nIn this step, we load the pretrained **Tiny Time Mixer** model for zero-shot prediction. A **zero-shot model** allows us to make predictions without any additional fine-tuning, leveraging the knowledge it has already acquired from pretraining on similar tasks.\n\n**Pretrained model:**\n\n- The `TinyTimeMixerForPrediction` class provides a pre-built architecture for time series forecasting, and we use the `from_pretrained` method to load the model. \n- Here, to save time, we use the same model that was trained on the dataset, namely `zero_shot_model`.\n","metadata":{}},{"cell_type":"code","source":"# The commented code loads the Tiny Time Mixer model for zero-shot predictions  from IBM's pre-trained repository (\"ibm/TTM\") with a revision of \"main\". \n# It also uses a prediction filter length of 24, meaning only 24 predicted values are shown per forecast window.\n\n# zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision=\"main\", prediction_filter_length=24)\n\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"models/zero_shot_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:10:04.758739Z","iopub.execute_input":"2024-12-08T02:10:04.759175Z","iopub.status.idle":"2024-12-08T02:10:04.822511Z","shell.execute_reply.started":"2024-12-08T02:10:04.759141Z","shell.execute_reply":"2024-12-08T02:10:04.821253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='evaluating-the-zero-shot-model'></a>[Evaluating the zero-shot model](#toc)\n\nOnce the pretrained zero-shot model is loaded, we use the `Trainer` class from the Hugging Face library to evaluate the model's performance on the test dataset. The `Trainer` class from Hugging Face simplifies the process of model training and evaluation by handling tasks such as forward passes, loss calculation, and metric evaluation.\n\nTrainer configuration:\n\n- **Model**: We pass the pretrained `zeroshot_model` to the `Trainer` class, which provides a high-level API for training and evaluating models.\n- **Test dataset**: The evaluation is performed on the test dataset that we generated earlier using the `get_datasets` method.\n\nEvaluation process:\n\nThe `evaluate()` method is used to assess the model's performance on the test set. Since this is a zero-shot model, the evaluation will give us insights into how well the model performs out-of-the-box, without additional training or customization.\n\nThis step helps us understand the initial forecasting capability of the model on our dataset.\n\nEvaluation will take **~2 minutes** to generate forecasts.\n","metadata":{}},{"cell_type":"code","source":"# zeroshot_trainer\nzeroshot_trainer = Trainer(\n    model=zeroshot_model,\n)\n\nzeroshot_trainer.evaluate(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:10:13.479993Z","iopub.execute_input":"2024-12-08T02:10:13.480411Z","iopub.status.idle":"2024-12-08T02:10:58.644306Z","shell.execute_reply.started":"2024-12-08T02:10:13.480372Z","shell.execute_reply":"2024-12-08T02:10:58.643169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='setting-up-the-time-series-forecasting-pipeline'></a>[Setting up the time series forecasting pipeline](#toc)\n\nTo make predictions using the zero-shot model, we configure a **Time Series Forecasting Pipeline**. This pipeline streamlines the process of generating forecasts by integrating the model and specifying key configurations.\n\n\n#### Purpose:\n\nThis pipeline simplifies the forecasting workflow, enabling us to efficiently generate predictions from our zero-shot model on the time series data. By configuring this pipeline, we prepare the model for making future predictions based on the input time series data.\n","metadata":{}},{"cell_type":"code","source":"# Set up the time series forecasting pipeline using the zero-shot model\nzs_forecast_pipeline = TimeSeriesForecastingPipeline(\n    # Use the zero-shot Tiny Time Mixer model\n    model=zeroshot_model,\n    # Specify the device (CPU in this case)\n    device=\"cpu\",\n    # Time reference column\n    timestamp_column=timestamp_column,\n    # No additional identifier columns\n    id_columns=[],\n    # Target variable (Close price)\n    target_columns=target_columns,\n    # Frequency of time series data (hourly)\n    freq=\"1h\"                     \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:11:04.006048Z","iopub.execute_input":"2024-12-08T02:11:04.006437Z","iopub.status.idle":"2024-12-08T02:11:04.014153Z","shell.execute_reply.started":"2024-12-08T02:11:04.006404Z","shell.execute_reply":"2024-12-08T02:11:04.012789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <a id='making-predictions-with-the-forecasting-pipeline'></a>[Making predictions with the forecasting pipeline](#toc)\n\nIn this step, we generate predictions using the **Time Series Forecasting Pipeline**. However, instead of recalculating the forecast, we will load a pre-generated forecast from a pickle file for efficiency.\n\n#### Explanation of the process:\n\n- **Commented code**: \n    - The commented code is used to generate predictions by passing the preprocessed test dataset to the forecasting pipeline.\n    - This approach would allow the zero-shot model to make predictions directly on the test data. Do not run it here, as it will take a lot of time.\n\n- **Loading pre-generated forecast**: \n    - Instead of running the pipeline again, we will load a pre-generated forecast from the `zs_forecast.pkl` file using `pd.read_pickle()`.\n\nBy loading the pre-generated forecast, we can immediately inspect the predictions made by the model.\n","metadata":{}},{"cell_type":"code","source":"# Run the forecasting pipeline on the preprocessed test dataset\n# This generates predictions for the 'Close' prices and stores the result in zs_forecast\n# zs_forecast contains both predicted ('Close_prediction') and actual ('Close') values\n\n# zs_forecast = zs_forecast_pipeline(tsp.preprocess(bt_data_resampled[test_start_index:test_end_index]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='understanding-the-forecast-process-and-missing-values'></a>[Understanding the forecast process and missing values](#toc)\n\n#### How the forecast works\n\nThe **Tiny Time Mixer (TTM)** model is used to forecast Bitcoin's `Close` prices. The model takes historical data to predict the next 24 time steps (hours), and stores these predictions in the `Close_prediction` column. Meanwhile, actual observed values for 96 time steps are stored in the `Close` column.\n\n#### Step-by-step forecast process:\n\n1. **Input data (t1 to t512)**: \n   - The model looks at the last **512 time points** (from `t1` to `t512`) as context, including values like `Open`, `High`, `Low`, and `Close`.\n\n2. **Model prediction (t513 to t609)**: \n   - Based on the past 512 data points, the model predicts **96 future points** (from `pt513` to `pt609`). These are the forecasted `Close` prices, which will be stored in the `Close_prediction` column.\n\n3. **Filtered prediction (t513 to t537)**: \n   - The model only shows **24 out of the 96 predicted points** (`pt513` to `pt537`) in the `Close_prediction` column due to the `prediction_filter_length=24` set during the model training. The rest of the 96 points are not shown.\n\n4. **Compare predicted t513 to t537 with actual t513 to t609**:\n   - The **predicted values** (from `pt513` to `pt537`) are compared with the **actual values** (from `t513` to `t537` that is first 24 out of 96 time steps). This allows for evaluation of the model’s accuracy for those 24 steps.\n\n#### Window shifts forward for the next prediction:\n\nAfter the first prediction, the window shifts by one timestamp:\n\n- **New input (t2 to t513)**: The model now looks at data from `t2` to `t513` and predicts the future steps.\n- **New prediction (t514 to t610)**: It predicts **96 future points** (from `pt514` to `pt610`).\n- **Filtered prediction (t514 to t538)**: Only the first **24 predicted points** (`pt514` to `pt538`) are shown in `Close_prediction` and are compared with actual values.\n\n#### Process repeats:\n\nThis sliding window repeats:\n\n- The model shifts one time point forward each time.\n- For each window, it predicts **96 values** and shows only **24 predictions**, comparing them against the actual values for the same time range.\n\n#### When does forecasting stop?\n\n- The forecasting process continues **until the end of the dataset**. Once the model reaches the end of the available historical data, it can no longer create a 512-point context window to predict further time steps.\n- For example, if the dataset has 10,000 time steps, the model will make predictions up to **t10,096** (with the last window using data from `t9,488` to `t10,000` as context).\n- After the model uses the final 512 data points available for input, it will no longer be able to predict the future since there’s no more data to feed into the model for additional context. At this point, the forecasting process stops.\n","metadata":{}},{"cell_type":"markdown","source":"![Forecast Process](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/SAv1DljdRu3JMRbLZV1TSg/Forecast%20Process.png)\n","metadata":{}},{"cell_type":"code","source":"zs_forecast = pd.read_pickle(\"models/zs_forecast.pkl\")\nzs_forecast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:11:31.304567Z","iopub.execute_input":"2024-12-08T02:11:31.304957Z","iopub.status.idle":"2024-12-08T02:11:31.453742Z","shell.execute_reply.started":"2024-12-08T02:11:31.304926Z","shell.execute_reply":"2024-12-08T02:11:31.452645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Why do `NaN` values occur toward the end of the forecast?\n\nThe `NaN` values in the `Close` column (actual values) occur because the actual data for future time steps is not yet available. Here’s how it works:\n\n- **Older rows (further back in the past)**: For rows that are further back in time, actual `Close` prices are available for 96 hours after the timestamp. Therefore, fewer `NaN` values are present in these rows because more of the future data is known.\n- **Recent rows (closer to the present)**: As you approach the present time, fewer future `Close` prices are available, leading to more `NaN` values. The most recent rows may have all 96 values as `NaN`, simply because those future time steps haven’t occurred yet, and thus, no actual data is available.\n\nFor example, consider a row with a timestamp of `2024-09-24 00:00:00`. If only a few future hours have passed, most of the 96 actual `Close` values will be `NaN` because we haven’t yet observed those future hours.\n\n#### Is this expected?\n\nYes, this is completely expected. In time series forecasting, the model predicts future values, but actual data for those time steps is only available as time progresses. The increasing number of `NaN` values in the `Close` column is a natural result of forecasting into the future, where the actual data isn’t available yet.\n\n#### Key takeaway\n\nThe model predicts the next 24 hours of `Close` prices based on historical data, while actual values for up to 96 hours are stored for comparison. Over time, these `NaN` values will be replaced with real data as those future hours occur, allowing for better comparison of predicted vs. actual values.\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='comparing-predicted-vs-actual-values'></a>[Comparing predicted vs actual values](#toc)\n\nWhen we look at the forecasted results, it's important to note that we have a window of 24 timestamps for predictions in the forecast object. This is because the model was configured to predict 24 time steps into the future for each prediction.\n\nTo compare the model's predictions to the actual values, we can plot both the predicted and actual **Close** prices for a given time window. In this example, we use **Row 11** from the forecast data to visualize the predicted versus actual values.\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='plotting-the-results'></a>[Plotting the results](#toc):\n- By plotting the **Predicted Values** and **Actual Values**, we can visually assess the accuracy of the model's predictions over the 24-timestep forecast window.\n","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame to compare the predicted and actual Close prices for Row 11\nfcast_df = pd.DataFrame({\n    \"pred\": zs_forecast.loc[11]['Close_prediction'],  # Predicted values for the next 24 time steps\n    \"actual\": zs_forecast.loc[11]['Close'][:24]       # Actual values for the same time period\n})\n\n# Plot the predicted vs actual Close prices\nax = fcast_df.plot()\n\n# Set labels and title for the plot\nax.set_xlabel(\"Time Steps\")  # Label for the X-axis (time steps)\nax.set_ylabel(\"Close Price\")  # Label for the Y-axis (Close price in USD)\nax.set_title(\"Predicted vs Actual Close Price for Row 11\")  # Title of the plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:11:43.995370Z","iopub.execute_input":"2024-12-08T02:11:43.995735Z","iopub.status.idle":"2024-12-08T02:11:44.369786Z","shell.execute_reply.started":"2024-12-08T02:11:43.995704Z","shell.execute_reply":"2024-12-08T02:11:44.368621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Row 11 analysis (above image)\n\nThe above image shows the predicted vs actual `Close` prices. In this case, the predictions for the next 24 time steps (hours) are compared to the actual values for the same period.\n\n- **Observation**: The actual values (orange line) fluctuate more significantly than the predicted values (blue line), which remain relatively steady. The model's predictions are higher and do not capture the sharp drops seen in the actual prices.\n  \n- **Explanation**: This discrepancy can be attributed to the fact that the model is trained to predict general trends and may not capture short-term fluctuations precisely. For **Row 11**, it appears the model missed the volatility of the actual data, resulting in a notable difference between the predicted and actual values.\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='evaluating-the-forecast-results-over-a-specific-time-horizon'></a>[Evaluating the forecast results over a specific time horizon](#toc)\n\nTo evaluate the model's performance over a specific time horizon, we can compare the predicted values to the actual values at a specified number of hours into the future. This custom function, `compare_forecast`, helps automate this comparison by focusing on a single prediction point, such as the value forecasted for the next day.\n\n#### Explanation of the function:\n\n- **Inputs**:\n  \n  - `forecast`: The forecast DataFrame containing the predictions and actual values.\n  - `date_col`: The name of the column containing the date or timestamp information.\n  - `prediction_col`: The column containing the model's forecasted values.\n  - `actual_col`: The column containing the actual observed values.\n  - `hours_out`: The number of hours into the future to compare (e.g., 24 hours for next-day prediction).\n  \n- **Process**:\n  \n  - The function initializes two lists (`actual` and `pred`) to store the actual and predicted values for the specified horizon (e.g., 24 hours).\n  - It loops through each row of the forecast data and extracts the value corresponding to `hours_out` from both the `prediction_col` and the `actual_col`.\n  - The result is a new DataFrame (`comparisons`) containing the timestamps, actual values, and predicted values for the specified horizon.\n  \n#### Output:\n\nThe function returns a DataFrame containing the timestamp, actual values, and predicted values for comparison at the specified future timestamp. This allows us to directly compare the model's forecast against the true observed value for a given prediction horizon.\n","metadata":{}},{"cell_type":"code","source":"def compare_forecast(forecast, date_col, prediction_col, actual_col, hours_out):\n    comparisons = pd.DataFrame()  # Initialize a new DataFrame to store comparisons\n    comparisons[date_col] = forecast[date_col]  # Store the date or timestamp column\n\n    # Initialize lists to store actual and predicted values\n    actual = []\n    pred = []\n\n    # Loop through the forecast data\n    for i in range(len(forecast)):\n        # Append the prediction for the specified hour (e.g., 24 hours into the future)\n        pred.append(forecast[prediction_col].values[i][hours_out - 1])  \n        # Append the actual value for the same hour\n        actual.append(forecast[actual_col].values[i][hours_out - 1])\n\n    # Add the actual and predicted values to the comparisons DataFrame\n    comparisons['actual'] = actual\n    comparisons['pred'] = pred\n\n    return comparisons  # Return the comparisons DataFrame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:11:52.951759Z","iopub.execute_input":"2024-12-08T02:11:52.952790Z","iopub.status.idle":"2024-12-08T02:11:52.960477Z","shell.execute_reply.started":"2024-12-08T02:11:52.952752Z","shell.execute_reply":"2024-12-08T02:11:52.959290Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above function allows us to align the forecasted values with the true observed values, and we can calculate the **Root Mean Squared Error (RMSE)** to assess the model's performance.\n\n#### Process:\n\n1. **Extract predictions and actual values**: We compare the predictions and actual values for 12 hours out by using the `compare_forecast` function.\n2. **Handle missing data**: Any rows where either `pred` or `actual` values contain `NaN` are dropped to ensure that the RMSE calculation only considers valid data points.\n3. **Calculate RMSE**: The **Root Mean Squared Error** is computed to evaluate how close the predicted values are to the actual values. RMSE is chosen because it works well with data containing small or negative values, providing a robust measure of prediction accuracy.\n4. **Plot Results**: The predicted and actual values are plotted against time, allowing us to visually compare the performance of the model.\n\n#### Why RMSE?\n\nWhile other metrics such as **Mean Absolute Error (MAE)** or **Mean Absolute Percentage Error (MAPE)** can be used, RMSE is particularly useful when the dataset contains very small or negative values, as it penalizes larger errors more heavily.\n\nThe plot gives us a visual representation of how well the model predicts the next 12 hours of Bitcoin **Close** prices.\n","metadata":{}},{"cell_type":"code","source":"# Get the predictions and actual values for 12 hours out\none_day_out_predictions = compare_forecast(zs_forecast, \"Timestamp\", \"Close_prediction\", \"Close\", 12)\n\n# Drop rows where either 'pred' or 'actual' contains NaN values to ensure valid data\nout = one_day_out_predictions.dropna(subset=[\"actual\", \"pred\"])\n\n# Calculate Root Mean Squared Error (RMSE)\nrms = '{:.10f}'.format(mean_squared_error(out['actual'], out['pred'], squared=False))\n\n# Print the RMSE result\nprint(f\"Root Mean Squared Error (RMSE): {rms}\")\n\n# Plot the predicted vs actual Close prices over time\nout.plot(x=\"Timestamp\", y=[\"pred\", \"actual\"], figsize=(20, 5), title=str(rms))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:12:02.095309Z","iopub.execute_input":"2024-12-08T02:12:02.095715Z","iopub.status.idle":"2024-12-08T02:12:02.681158Z","shell.execute_reply.started":"2024-12-08T02:12:02.095682Z","shell.execute_reply":"2024-12-08T02:12:02.679899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Overall trend analysis (above image)\n\nThe above image plots the **predicted vs actual `Close` prices over time**, across the entire dataset. The **Root Mean Squared Error (RMSE)** for this forecast is `0.0663050094`.\n\n- **Observation**: Although there are differences between the predicted and actual values at each time step, the model does a good job of capturing the **overall trend** of the data. The predicted values (blue line) closely follow the actual values (orange line), with both showing similar directional changes over time.\n\n- **Explanation**: Despite missing some short-term fluctuations (like in Row 11), the model captures the broader **upward and downward trends** over longer periods. This indicates that while the model may not be perfect for short-term predictions, it still has the ability to capture the **general direction** of the price movements over time.\n\n#### Key takeaway\n\n- The model may struggle with predicting exact short-term price movements, as seen in **Row 11**, where the predicted values are quite different from the actual ones. \n- However, when evaluated over the entire dataset, the model successfully follows the general trend, even if there are some discrepancies at individual points. This shows that while the model may not always capture short-term volatility, it performs well in forecasting the overall movement of Bitcoin's `Close` prices.\n  \nOverall, this is expected behavior in time series forecasting, where capturing the broader trend is often more important than predicting exact values for each time step.\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='fine-tuning-the-ttm-model'></a>[Fine tuning the TTM model](#toc)\n\nNow let's fine-tune the model weights using our training data and evaluate the model on the test set. To do this, we need to configure the **Training Arguments**.\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='setting-up-parameters-for-fine-tuning'></a>[Setting up parameters for fine tuning](#toc)\n\n#### Key parameters:\n\n- **Learning Rate**: The learning rate controls how much the model weights are updated during each iteration. In this case, we set it to `0.0001`.\n- **Number of Epochs**: The model will be trained for `10` epochs.\n- **Batch Size**: Both the training and evaluation batch size are set to `32`, determining how many samples are processed at a time.\n- **Early Stopping**: The model will load the best version at the end of training, based on the lowest evaluation loss (`eval_loss`), ensuring we use the best-performing model for predictions.\n\n#### Additional configuration:\n\n- **Logging and Saving**: Logs and model checkpoints will be saved at the end of each epoch, and only the best model (based on `eval_loss`) will be kept to save space.\n- **Parallelism**: The `dataloader_num_workers=8` parameter is set to improve the efficiency of data loading during training.\n\nThese training arguments ensure that the model is fine-tuned efficiently, with mechanisms in place to monitor and save the best model based on its performance.\n","metadata":{}},{"cell_type":"code","source":"OUT_DIR = \"\"\n\n# Important parameters for fine-tuning\nlearning_rate = 0.0001  # Set the learning rate for weight updates\nnum_epochs = 10  # Number of training epochs\nbatch_size = 32  # Number of samples processed in each batch\n\n# Configure the TrainingArguments for fine-tuning\nfinetune_forecast_args = TrainingArguments(\n    output_dir=os.path.join(OUT_DIR, \"output\"),  # Directory to save model checkpoints and outputs\n    overwrite_output_dir=True,  # Overwrite existing output directory\n    learning_rate=learning_rate,  # Learning rate for the optimizer\n    num_train_epochs=num_epochs,  # Total number of training epochs\n    do_eval=True,  # Perform evaluation at the end of each epoch\n    eval_strategy=\"epoch\",  # Evaluate at the end of every epoch\n    per_device_train_batch_size=batch_size,  # Training batch size\n    per_device_eval_batch_size=batch_size,  # Evaluation batch size\n    dataloader_num_workers=8,  # Number of workers for data loading\n    save_strategy=\"epoch\",  # Save model at the end of every epoch\n    logging_strategy=\"epoch\",  # Log metrics at the end of every epoch\n    save_total_limit=1,  # Keep only the best model to save space\n    logging_dir=os.path.join(OUT_DIR, \"logs\"),  # Directory to store logs\n    load_best_model_at_end=True,  # Load the best model based on evaluation loss at the end\n    metric_for_best_model=\"eval_loss\",  # Monitor evaluation loss to choose the best model\n    greater_is_better=False,  # For loss metrics, lower is better\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:12:19.686832Z","iopub.execute_input":"2024-12-08T02:12:19.687255Z","iopub.status.idle":"2024-12-08T02:12:19.698252Z","shell.execute_reply.started":"2024-12-08T02:12:19.687218Z","shell.execute_reply":"2024-12-08T02:12:19.697141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='loading-the-fine-tuned-forecast-model'></a>[Loading the fine-tuned forecast model](#toc)\n\nBefore we proceed with fine-tuning, we load the previously fine-tuned **Tiny Time Mixer (TTM)** model using `from_pretrained()`. This allows us to initialize the model with weights that were adjusted during an earlier fine-tuning process.\n\n#### Purpose:\n\n- **Fine-tuned Model**: Unlike the zero-shot model, which makes predictions without further training, the fine-tuned model has been trained on our specific time series data. This allows it to generate more accurate forecasts that better reflect the patterns in the Bitcoin dataset.\n  \nThis step is crucial before configuring the training process, as we need a model instance to pass to the **Trainer**.\n","metadata":{}},{"cell_type":"code","source":"finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\"models/finetuned_forecast_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:12:27.082790Z","iopub.execute_input":"2024-12-08T02:12:27.083230Z","iopub.status.idle":"2024-12-08T02:12:27.141563Z","shell.execute_reply.started":"2024-12-08T02:12:27.083195Z","shell.execute_reply":"2024-12-08T02:12:27.140394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once the fine-tuned model is loaded, we set up important components for training, such as early stopping, tracking, the optimizer, and the learning rate scheduler.\n\nTo prevent the model from overfitting during fine-tuning, we implement **early stopping**. Additionally, we configure the **optimizer** and **scheduler** that will be used during training to adjust the model's learning rate dynamically.\n","metadata":{}},{"cell_type":"code","source":"# Create the early stopping callback to prevent overfitting\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=2,  # Stop training after 2 epochs of no improvement\n    early_stopping_threshold=0.001,  # Minimum improvement required to continue training\n)\n\n# Callback for tracking model training\ntracking_callback = TrackingCallback()\n\n# Configure the optimizer (AdamW) and learning rate scheduler (OneCycleLR)\noptimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)  # Optimizer for weight updates\nscheduler = OneCycleLR(\n    optimizer,  \n    learning_rate,  \n    epochs=num_epochs,  \n    steps_per_epoch=math.ceil(len(train_dataset) / (batch_size)),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:12:39.872401Z","iopub.execute_input":"2024-12-08T02:12:39.872789Z","iopub.status.idle":"2024-12-08T02:12:39.881577Z","shell.execute_reply.started":"2024-12-08T02:12:39.872757Z","shell.execute_reply":"2024-12-08T02:12:39.880210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once we have configured all the training parameters, we can begin fine-tuning our model using the **Trainer** class from the Hugging Face library.\n","metadata":{}},{"cell_type":"code","source":"# Configure the Trainer for fine-tuning the model\nfinetune_forecast_trainer = Trainer(\n    model=finetune_forecast_model,  # Fine-tuned version of the Tiny Time Mixer model\n    args=finetune_forecast_args,  # Training arguments (learning rate, batch size, epochs, etc.)\n    train_dataset=train_dataset,  # Training dataset\n    eval_dataset=valid_dataset,  # Validation dataset for evaluation during training\n    callbacks=[early_stopping_callback, tracking_callback],  # Early stopping and tracking callbacks\n    optimizers=(optimizer, scheduler),  # Optimizer (AdamW) and learning rate scheduler (OneCycleLR)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:12:48.974174Z","iopub.execute_input":"2024-12-08T02:12:48.974730Z","iopub.status.idle":"2024-12-08T02:12:48.991293Z","shell.execute_reply.started":"2024-12-08T02:12:48.974677Z","shell.execute_reply":"2024-12-08T02:12:48.990153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After setting up the model, optimizer, scheduler, and trainer, the training process begins by calling the `train()` method. However, to ease the process, we just use the model already trained and load the fine tuned forecast in the later steps.\n","metadata":{}},{"cell_type":"code","source":"# finetune_forecast_trainer.train()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After fine-tuning the Tiny Time Mixer model, we configure the **Time Series Forecasting Pipeline** to generate predictions using the newly fine-tuned model. The pipeline automates the forecasting process, enabling us to efficiently make predictions based on the input data.\n","metadata":{}},{"cell_type":"code","source":"forecast_pipeline = TimeSeriesForecastingPipeline(\n    model=finetune_forecast_model,\n    device=\"cpu\",\n    timestamp_column=timestamp_column,\n    id_columns=[],\n    target_columns=target_columns,\n    observable_columns=observable_columns,\n    freq=\"1h\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:13:01.594533Z","iopub.execute_input":"2024-12-08T02:13:01.594979Z","iopub.status.idle":"2024-12-08T02:13:01.601905Z","shell.execute_reply.started":"2024-12-08T02:13:01.594943Z","shell.execute_reply":"2024-12-08T02:13:01.600735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <a id='evaulating-fine-tuned-forecast'></a>[Evaluating fine-tuned forecast](#toc)\n\nAfter fine-tuning the model, we can evaluate its performance on the test dataset. The commented-out section shows how we would run the evaluation process using the **Trainer** class, but for efficiency, we load pre-generated evaluation results from a pickle file using `pd.read_pickle()` to save time and computational resources.\n\nBy loading the fine-tuned forecast, we can immediately evaluate the model's predictions and compare them to the actual values.\n","metadata":{}},{"cell_type":"code","source":"# finetune_forecast_trainer.evaluate(tsp.preprocess(bt_data_resampled[test_start_index:test_end_index]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecast_finetuned = pd.read_pickle(\"models/forecast_finetuned.pkl\")\nforecast_finetuned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:13:18.545576Z","iopub.execute_input":"2024-12-08T02:13:18.546004Z","iopub.status.idle":"2024-12-08T02:13:18.683583Z","shell.execute_reply.started":"2024-12-08T02:13:18.545967Z","shell.execute_reply":"2024-12-08T02:13:18.682456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To assess the performance of the fine-tuned model, we can compare its predictions with the actual values for a specific row in the test dataset. In this case, we focus on **Row 11**, and we plot the predicted and actual **Close** prices for the next 24 time steps.\n","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame to compare the predicted and actual Close prices for Row 11\nfcast_df = pd.DataFrame({\n    \"pred\": forecast_finetuned.loc[11]['Close_prediction'],  # Predicted Close prices for the next 24 time steps\n    \"actual\": forecast_finetuned.loc[11]['Close'][:24]       # Actual Close prices for the same period\n})\n\n# Plot the predicted vs actual Close prices\nax = fcast_df.plot()\n\n# Set labels and title for the plot\nax.set_xlabel(\"Time Steps\")  # X-axis represents the time steps (hours)\nax.set_ylabel(\"Close Price\")  # Y-axis represents the Close price in USD\nax.set_title(\"Predicted vs Actual Close Price for Row 11\")  # Title of the plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:13:27.164689Z","iopub.execute_input":"2024-12-08T02:13:27.165077Z","iopub.status.idle":"2024-12-08T02:13:27.439630Z","shell.execute_reply.started":"2024-12-08T02:13:27.165045Z","shell.execute_reply":"2024-12-08T02:13:27.438513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can further evaluate the model by comparing the predicted values with the actual values at a specific time horizon, in this case, **12 hours out**. The following steps involve comparing the predictions, calculating the **Root Mean Squared Error (RMSE)**, and plotting the predictions against the actual values.\n","metadata":{}},{"cell_type":"code","source":"# Get the predictions and actual values for 12 hours out (you can change this to other values if needed)\nforecast_predictions = compare_forecast(forecast_finetuned, \"Timestamp\", \"Close_prediction\", \"Close\", 12)\n\n# Drop rows where either 'pred' or 'actual' contains NaN values to ensure valid data for RMSE calculation\nforecast_out = forecast_predictions.dropna(subset=[\"actual\", \"pred\"])\n\n# Calculate Root Mean Squared Error (RMSE) between predicted and actual values\nrms = '{:.10f}'.format(mean_squared_error(forecast_out['actual'], forecast_out['pred'], squared=False))\n\n# Print the calculated RMSE\nprint(f\"Root Mean Squared Error (RMSE): {rms}\")\n\n# Plot the predicted vs actual values over time, with RMSE in the title\nforecast_out.plot(x=\"Timestamp\", y=[\"pred\", \"actual\"], figsize=(20, 5), title=f\"RMSE: {rms}\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:13:35.931293Z","iopub.execute_input":"2024-12-08T02:13:35.931756Z","iopub.status.idle":"2024-12-08T02:13:36.494509Z","shell.execute_reply.started":"2024-12-08T02:13:35.931721Z","shell.execute_reply":"2024-12-08T02:13:36.493374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <a id='conclusions-from-zero-shot-and-fine-tuned-model-performance'></a>[Conclusions from zero-shot and fine-tuned model performance](#toc)\n\n### Zero-shot learning (first image)\n\nIn the first image, we observe the performance of the model before any fine-tuning, known as **zero-shot learning**. The **Root Mean Squared Error (RMSE)** is shown to be approximately **0.0663**, which indicates that the model performs reasonably well out-of-the-box, capturing the general trend of the actual values over time. However, several key observations can be made:\n\n- The **predicted values** (blue line) closely follow the **actual values** (orange line), especially in areas with less volatility.\n- However, in regions with sharp changes or fluctuations, the model struggles to accurately predict the peaks and troughs of the actual values.\n- Despite this, the model captures the overall **directional trend** well, indicating that the underlying architecture of the model is able to forecast the broader patterns in the data.\n\n### Fine-tuned model (second image)\n\nIn the second image, we see the results after **fine-tuning** the model on the training dataset. The **RMSE** has improved to **0.0655**, reflecting better overall performance. Key improvements include:\n\n- The fine-tuned model significantly reduces the prediction error, especially in areas where the zero-shot model showed a gap between the predicted and actual values.\n- The **predicted line** (blue) more closely aligns with the **actual values** (orange), particularly in regions with more volatility and sudden changes in the time series data.\n- Fine-tuning allows the model to better adapt to the specific characteristics of the dataset, improving its ability to generalize and capture complex fluctuations that the zero-shot model missed.\n\n### Overall conclusion\n\nThe fine-tuned model outperforms the zero-shot model in terms of **RMSE** and visual alignment of predicted and actual values. The fine-tuning process allowed the model to:\n\n- **Reduce prediction error** across the board, with particularly noticeable improvements in volatile regions.\n- **Better capture short-term fluctuations** in the time series while maintaining accuracy on long-term trends.\n- Achieve an overall **better fit** to the actual data, as indicated by the reduction in RMSE from **0.0663** to **0.0655**.\n\nIn conclusion, fine-tuning the Tiny Time Mixer model significantly improves its performance, making it better suited for real-world forecasting tasks that involve complex and volatile time series data.\n","metadata":{}},{"cell_type":"markdown","source":"## [Time series forecasting using the TSFM model](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"\n\n We will generate a synthetic time series dataset and use it to train the **Tiny Time Mixer (TTM)** model for time series forecasting. The model will be fine-tuned to predict future values based on historical data, and we will evaluate its performance using metrics such as **Root Mean Squared Error (RMSE)**.\n","metadata":{}},{"cell_type":"markdown","source":"### Objectives\nBy the end of this exercise, you will be able to:\n1. **Create a time series dataset** using sine wave data.\n2. **Set up a time series forecasting model** using the **Tiny Time Mixer (TTM)** model.\n3. **Split the dataset** into training, validation, and testing sets for model training.\n4. **Train the model** using the **Trainer** class from Hugging Face.\n5. **Make predictions** on unseen test data and evaluate the performance using **RMSE**.\n\n### Dataset description\n\nThe dataset is generated using a combination of a sine wave and random noise to simulate fluctuations over time. The dataset consists of the following columns:\n\n- **Timestamp**: Represents hourly intervals starting from '2023-01-01'.\n- **Value**: A time series value generated from a sine function with added noise.\n","metadata":{}},{"cell_type":"code","source":"timestamps = pd.date_range('2023-01-01', periods=1000, freq='h')\nvalues = np.sin(np.linspace(0, 100, 1000)) + np.random.normal(0, 0.1, 1000)\n\n# Create the DataFrame\ndf = pd.DataFrame({'Timestamp': timestamps, 'Value': values})\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:14:42.934285Z","iopub.execute_input":"2024-12-08T02:14:42.934721Z","iopub.status.idle":"2024-12-08T02:14:42.953179Z","shell.execute_reply.started":"2024-12-08T02:14:42.934687Z","shell.execute_reply":"2024-12-08T02:14:42.951647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [ Split dataset for time series forecasting](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"#### **Objective:**\n\nIn this exercise, you will learn how to split a time series dataset into train, validation, and test sets. These splits are critical for training a machine learning model, validating its performance, and testing how well the model generalizes to unseen data.\n\n#### **Instructions:**\n\n1. **Define parameters:**\n   You will first define the necessary parameters for your time series model. This includes specifying the column that contains timestamps and the column that contains the target values you want to predict.\n\n   - The column that contains the timestamps is `\"Timestamp\"`.\n   - The column that contains the target values is `\"Value\"`.\n\n2. **Set the random seed:**\n   To ensure reproducibility in your results, set a seed value using `set_seed(SEED)` where `SEED` is predefined.\n\n3. **Set forecasting parameters:**\n   - `context_length`: For this exercise, set the context length to 512 time points.\n   - `forecast_length`: Set the forecast length is 96 time points.\n\n4. **Dataset length:**\n   Retrieve the length of the dataset using `data_length = len(df)`.\n\n5. **Dataset splitting:**\n   Define indices to split the dataset into training (70%), validation (10%), and testing (20%) sets. \n   \n6. **Split configuration:**\n   Store the indices for each dataset split in the `split_config` dictionary. You’ll need to ensure that your validation and test sets include the context length so that the model can make predictions.\n\n7. **Output:**\n   Print the start and end indices for the train, validation, and test sets to verify the dataset splits.\n","metadata":{}},{"cell_type":"code","source":"# Step 1: Set up parameters\ntimestamp_column = \"Timestamp\"\ntarget_column = [\"Value\"]\n\n# Set seed for reproducibility\nSEED = 42\nset_seed(SEED)\n\n# Forecasting parameters\ncontext_length = 512  # Use 512 time points from the past\nforecast_length = 96   # Predict 96 time points into the future\n\n# Step 2: Get the length of the dataset\ndata_length = len(df)\n\n# Step 3: Define the indices for the train, validation, and test splits\ntrain_start_index = 0\ntrain_end_index = round(data_length * 0.7)  # First 70% for training\neval_start_index = round(data_length * 0.7) - context_length  # Next 10% for validation\neval_end_index = round(data_length * 0.8)\ntest_start_index = round(data_length * 0.8) - context_length  # Final 20% for testing\ntest_end_index = data_length\n\n# Store the split configuration\nsplit_config = {\n    \"train\": [train_start_index, train_end_index],\n    \"valid\": [eval_start_index, eval_end_index],\n    \"test\": [test_start_index, test_end_index],\n}\n\n# Print the split indices\nprint(f\"Train: {train_start_index} to {train_end_index}\")\nprint(f\"Validation: {eval_start_index} to {eval_end_index}\")\nprint(f\"Test: {test_start_index} to {test_end_index}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:16:20.547902Z","iopub.execute_input":"2024-12-08T02:16:20.548337Z","iopub.status.idle":"2024-12-08T02:16:20.562522Z","shell.execute_reply.started":"2024-12-08T02:16:20.548290Z","shell.execute_reply":"2024-12-08T02:16:20.561276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [ Preprocess and create dataset](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"#### **Objective:*\nYou will preprocess the time series data and create the train, validation, and test datasets. Preprocessing ensures the data is in the correct format and scaled appropriately for model training. You will also split the preprocessed data into the train, validation, and test sets based on the configuration from the previous exercise.\n\n#### **Instructions:**\n\n1. **Define column specifiers:**\n   \n   The first step is to define the column specifiers that tell the preprocessor which columns to use for timestamps and target values.\n   \n   - `timestamp_column`: The column containing timestamps (e.g., `\"Timestamp\"`).\n   - `target_columns`: The column containing the target values you want to predict (e.g., `\"Value\"`).\n\n   You will store these in a dictionary named `column_specifiers`.\n\n2. **Initialize the preprocessor:**\n   \n   You will use the `TimeSeriesPreprocessor` to initialize the preprocessing pipeline. The preprocessor will ensure that the data is appropriately scaled and structured for model training.\n\n3. **Preprocess the data:**\n   \n   Using the `get_datasets()` function, apply the preprocessor to the dataset (`df`) and split the data into train, validation, and test sets based on the configuration created in **Exercise 1**. The function will return three datasets:\n   \n   - `train_dataset`: Contains the training data.\n   - `valid_dataset`: Contains the validation data.\n   - `test_dataset`: Contains the test data.\n","metadata":{}},{"cell_type":"code","source":"# Step 1: Define the column specifiers for time series data\ncolumn_specifiers = {\n    \"timestamp_column\": timestamp_column,\n    \"target_columns\": target_column,\n}\n\n# Step 2: Initialize the preprocessor\ntsp = TimeSeriesPreprocessor(\n    **column_specifiers,\n    context_length=context_length,\n    prediction_length=forecast_length,\n    scaling=True,\n    encode_categorical=False,\n    scaler_type=\"standard\",\n)\n\n# Step 3: Preprocess the data and get the train, validation, and test datasets\ntrain_dataset, valid_dataset, test_dataset = get_datasets(\n    tsp, df, split_config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:18:01.227287Z","iopub.execute_input":"2024-12-08T02:18:01.227702Z","iopub.status.idle":"2024-12-08T02:18:01.819185Z","shell.execute_reply.started":"2024-12-08T02:18:01.227666Z","shell.execute_reply":"2024-12-08T02:18:01.817519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [ Train the model](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"You will train a `TinyTimeMixerForPrediction` model on the preprocessed train dataset. You will set up the training configuration, define the optimizer and scheduler, and train the model using the `Trainer` from Hugging Face's Transformers library.\n\n#### **Instructions:**\n\n1. **Set important parameters:**\n   You will start by defining key parameters for model training such as learning rate, number of epochs and batch size.\n\n2. **Set up training arguments:**\n   Define the training configuration using `TrainingArguments`. Key arguments include:\n   \n   - `output_dir`: Directory where model outputs will be saved.\n   - `learning_rate`: The learning rate for the training process.\n   - `num_train_epochs`: Number of epochs for training (set to 10).\n   - `eval_strategy`: Perform evaluation at the end of each epoch.\n   - `save_strategy`: Save model checkpoints after each epoch.\n   - `logging_dir`: Directory where training logs will be saved.\n   - `metric_for_best_model`: Use `eval_loss` to track and save the best model.\n     \n\n3. **Load the model:**\n   Load the `TinyTimeMixerForPrediction` model from IBM’s pretrained model repository. This will serve as the starting point for training.\n\n4. **Set up callbacks:**\n   Create two callbacks to monitor the training process:\n   \n   - `early_stopping_callback`: Set it to 2 in this case.\n   - `tracking_callback`: Tracks the training process.\n\n5. **Optimizer and scheduler:**\n   Define the optimizer (`AdamW`) and the learning rate scheduler (`OneCycleLR`).\n\n6. **Set up the trainer:**\n   Use the `Trainer` class to bring together the model, training arguments, optimizer, scheduler, datasets, and callbacks for training.\n\n7. **Train the model:**\n   Train the model using the `.train()` method.\n","metadata":{}},{"cell_type":"code","source":"OUT_DIR = \"ttm_trained_models_practice/\"\n\n# Important parameters\nlearning_rate = 0.0001\nnum_epochs = 10\nbatch_size = 32\n\n# Step 1: Set up training arguments\ntrain_forecast_args = TrainingArguments(\n    output_dir=os.path.join(OUT_DIR, \"output\"),\n    overwrite_output_dir=True,\n    learning_rate=learning_rate,\n    num_train_epochs=num_epochs,\n    do_eval=True,\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    dataloader_num_workers=8,\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_dir=os.path.join(OUT_DIR, \"logs\"),\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\n# Step 2: Load the model for training\ntrain_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n    \"ibm/TTM\", revision=\"main\", prediction_filter_length=24\n)\n\n# Step 3: Create early stopping callback and tracking callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=2,\n    early_stopping_threshold=0.001,\n)\ntracking_callback = TrackingCallback()\n\n# Optimizer and scheduler\noptimizer = AdamW(train_forecast_model.parameters(), lr=learning_rate)\nscheduler = OneCycleLR(\n    optimizer,\n    learning_rate,\n    epochs=num_epochs,\n    steps_per_epoch=math.ceil(len(train_dataset) / batch_size),\n)\n\n# Step 4: Set up the Trainer for training\ntrain_forecast_trainer = Trainer(\n    model=train_forecast_model,\n    args=train_forecast_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    callbacks=[early_stopping_callback, tracking_callback],\n    optimizers=(optimizer, scheduler),\n)\n\n# Step 5: Train the model\ntrain_forecast_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:19:22.158643Z","iopub.execute_input":"2024-12-08T02:19:22.159074Z","iopub.status.idle":"2024-12-08T02:19:30.104482Z","shell.execute_reply.started":"2024-12-08T02:19:22.159038Z","shell.execute_reply":"2024-12-08T02:19:30.103198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [ Make predictions and evaluating the model](#toc)\n","metadata":{}},{"cell_type":"markdown","source":"You will use the trained model to make predictions on the test dataset. You will then evaluate the model’s performance by comparing the predicted values with the actual values using the Root Mean Squared Error (RMSE) metric.\n\n#### **Instructions:**\n\n1. **Preprocess the test data:**\n   \n   - Use the `TimeSeriesPreprocessor` (tsp) to preprocess the test portion of the dataset. \n   - Select the appropriate data points based on the test indices defined earlier (`test_start_index` to `test_end_index`).\n   - The preprocessed data will be used to generate forecasts.\n\n2. **Set up the forecasting Pipeline:**\n   \n   - Create a `TimeSeriesForecastingPipeline` using the trained model.\n   - Set the `device` to `\"cpu\"` to perform inference on the CPU.\n   - Define the column names for required columns.\n   - Set the frequency of the time series for hourly data.\n\n4. **Make forecasts on the test dataset:**\n   \n5. **Compare forecasts and calculate RMSE:**\n   \n   - Compare the predicted values from the model with the actual values in the test set using the `compare_forecast()` function.\n   - Calculate the Root Mean Squared Error (RMSE) to quantify the error between the predicted and actual values.\n\n7. **Plot predictions vs actual values:**\n   \n   - Plot the predicted values against the actual values over time using `matplotlib`.\n","metadata":{}},{"cell_type":"code","source":"# Step 1: Preprocess the test data\ntest_data = tsp.preprocess(df[test_start_index:test_end_index])\n\n# Step 2: Set up the forecasting pipeline with the trained model\nforecast_pipeline = TimeSeriesForecastingPipeline(\n    model=train_forecast_model,  # Use the trained model here\n    device=\"cpu\",\n    timestamp_column=timestamp_column,\n    id_columns=[],\n    target_columns=target_column,\n    freq=\"1h\"\n)\n\n# Step 3: Make forecasts on the test dataset\nforecasts = forecast_pipeline(test_data)\n\n# Step 4: Compare forecasts and calculate RMSE\nforecast_predictions = compare_forecast(forecasts, \"Timestamp\", \"Value_prediction\", \"Value\", 12)\n\n# Drop rows with NaN values\nforecast_out = forecast_predictions.dropna(subset=[\"actual\", \"pred\"])\n\n# Calculate RMSE\nrms = '{:.10f}'.format(mean_squared_error(forecast_out['actual'], forecast_out['pred'], squared=False))\nprint(f\"Root Mean Squared Error (RMSE): {rms}\")\n\n# Step 5: Plot the predictions vs actual values\nforecast_out.plot(x=\"Timestamp\", y=[\"pred\", \"actual\"], figsize=(20, 5), title=f\"RMSE: {rms}\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T02:20:54.673118Z","iopub.execute_input":"2024-12-08T02:20:54.673562Z","iopub.status.idle":"2024-12-08T02:20:56.461749Z","shell.execute_reply.started":"2024-12-08T02:20:54.673528Z","shell.execute_reply":"2024-12-08T02:20:56.460850Z"}},"outputs":[],"execution_count":null}]}